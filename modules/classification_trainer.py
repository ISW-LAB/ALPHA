# modules/classification_trainer.py
"""
Classification ëª¨ë¸ í•™ìŠµ ëª¨ë“ˆ
ìˆ˜ë™ ë¼ë²¨ë§ëœ ê°ì²´ ë°ì´í„°ë¡œ DenseNet ë¶„ë¥˜ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ëŠ” ëª¨ë“ˆ
"""

import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, Subset
from torchvision import models, transforms
from PIL import Image
import numpy as np
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import time
import random
from tqdm import tqdm
from torch.optim.lr_scheduler import ReduceLROnPlateau
import matplotlib.pyplot as plt
import json

class ObjectDataset(Dataset):
    """ê°ì²´ ì´ë¯¸ì§€ ë°ì´í„°ì…‹ í´ë˜ìŠ¤"""
    
    def __init__(self, class0_dir, class1_dir, transform=None):
        """
        ê°ì²´ ì´ë¯¸ì§€ ë°ì´í„°ì…‹ ì´ˆê¸°í™”
        
        Args:
            class0_dir (str): í´ë˜ìŠ¤ 0 ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ ê²½ë¡œ
            class1_dir (str): í´ë˜ìŠ¤ 1 ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ ê²½ë¡œ
            transform: ì´ë¯¸ì§€ ë³€í™˜ê¸°
        """
        self.transform = transform
        self.class_dirs = [class0_dir, class1_dir]
        self.samples = []
        
        # ê° í´ë˜ìŠ¤ ë””ë ‰í† ë¦¬ì—ì„œ ì´ë¯¸ì§€ ë¡œë“œ
        for class_idx, class_dir in enumerate(self.class_dirs):
            if not os.path.isdir(class_dir):
                print(f"âš ï¸ ê²½ê³ : {class_dir} ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                continue
                
            image_count = 0
            for img_name in os.listdir(class_dir):
                if img_name.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):
                    img_path = os.path.join(class_dir, img_name)
                    try:
                        # ì´ë¯¸ì§€ íŒŒì¼ì´ ì˜¬ë°”ë¥¸ì§€ í™•ì¸
                        with Image.open(img_path) as img:
                            img.verify()
                        self.samples.append((img_path, class_idx))
                        image_count += 1
                    except Exception as e:
                        print(f"âš ï¸ ì†ìƒëœ ì´ë¯¸ì§€ íŒŒì¼ ê±´ë„ˆëœ€: {img_path} - {str(e)}")
            
            print(f"í´ë˜ìŠ¤ {class_idx}: {image_count}ê°œ ì´ë¯¸ì§€ ë¡œë“œë¨ ({class_dir})")
        
        print(f"ğŸ“Š ì´ {len(self.samples)}ê°œì˜ ì´ë¯¸ì§€ê°€ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        if len(self.samples) == 0:
            raise ValueError("ë¡œë“œëœ ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤. ë””ë ‰í† ë¦¬ ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        """
        ë°ì´í„°ì…‹ì—ì„œ í•­ëª© ê°€ì ¸ì˜¤ê¸°
        
        Args:
            idx (int): ì¸ë±ìŠ¤
            
        Returns:
            tuple: (ì´ë¯¸ì§€, í´ë˜ìŠ¤ ë¼ë²¨)
        """
        img_path, label = self.samples[idx]
        
        try:
            image = Image.open(img_path).convert('RGB')
        except Exception as e:
            print(f"âš ï¸ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {img_path} - {str(e)}")
            # ì˜¤ë¥˜ ì‹œ ë¹ˆ ì´ë¯¸ì§€ ë°˜í™˜
            image = Image.new('RGB', (224, 224), color='black')
        
        if self.transform:
            image = self.transform(image)
            
        return image, label

class ClassificationTrainer:
    """Classification ëª¨ë¸ í•™ìŠµê¸°"""
    
    def __init__(self, class0_dir, class1_dir, output_dir, batch_size=16, 
                 num_epochs=30, gpu_num=0, random_seed=13):
        """
        Classification í•™ìŠµê¸° ì´ˆê¸°í™”
        
        Args:
            class0_dir (str): í´ë˜ìŠ¤ 0 ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬
            class1_dir (str): í´ë˜ìŠ¤ 1 ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬
            output_dir (str): ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
            batch_size (int): ë°°ì¹˜ í¬ê¸°
            num_epochs (int): í•™ìŠµ ì—í­ ìˆ˜
            gpu_num (int): ì‚¬ìš©í•  GPU ë²ˆí˜¸
            random_seed (int): ëœë¤ ì‹œë“œ
        """
        self.class0_dir = class0_dir
        self.class1_dir = class1_dir
        self.output_dir = output_dir
        self.batch_size = batch_size
        self.num_epochs = num_epochs
        self.gpu_num = gpu_num
        self.random_seed = random_seed
        
        # ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ì„¤ì •
        self.set_seed()
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        self.device = torch.device(f"cuda:{self.gpu_num}" if torch.cuda.is_available() else "cpu")
        print(f"ğŸ”§ ì‚¬ìš© ì¥ì¹˜: {self.device}")
        
        # ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(self.output_dir, exist_ok=True)
        
        # ì´ë¯¸ì§€ ë³€í™˜ê¸° ì •ì˜
        self.data_transforms = {
            'train': transforms.Compose([
                transforms.Resize((224, 224)),
                transforms.RandomHorizontalFlip(p=0.5),  # ë°ì´í„° ì¦ê°•
                transforms.RandomRotation(degrees=10),    # íšŒì „ ì¦ê°•
                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # ìƒ‰ìƒ ì¦ê°•
                transforms.ToTensor(),
                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
            ]),
            'val': transforms.Compose([
                transforms.Resize((224, 224)),
                transforms.ToTensor(),
                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
            ]),
        }
        
        print("âœ… Classification Trainer ì´ˆê¸°í™” ì™„ë£Œ")
    
    def set_seed(self):
        """ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ì„¤ì •"""
        torch.manual_seed(self.random_seed)
        torch.cuda.manual_seed(self.random_seed)
        torch.cuda.manual_seed_all(self.random_seed)
        np.random.seed(self.random_seed)
        random.seed(self.random_seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
    
    def init_weights(self, m):
        """ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” í•¨ìˆ˜"""
        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):
            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            if m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.BatchNorm2d):
            nn.init.constant_(m.weight, 1)
            nn.init.constant_(m.bias, 0)
    
    def create_model(self):
        """DenseNet121 ëª¨ë¸ ìƒì„±"""
        print("ğŸ—ï¸ DenseNet121 ëª¨ë¸ ìƒì„± ì¤‘...")
        
        # DenseNet121 ëª¨ë¸ ì´ˆê¸°í™” (ì‚¬ì „ í›ˆë ¨ëœ ê°€ì¤‘ì¹˜ ì‚¬ìš©)
        model = models.densenet121(pretrained=True)
        
        # íŠ¹ì§• ì¶”ì¶œ ë ˆì´ì–´ ëŒ€ë¶€ë¶„ ê³ ì • (ê³¼ì í•© ë°©ì§€)
        for param in model.features.parameters():
            param.requires_grad = False
            
        # ë§ˆì§€ë§‰ dense blockë§Œ í•™ìŠµ ê°€ëŠ¥í•˜ê²Œ ì„¤ì •
        for param in model.features.denseblock4.parameters():
            param.requires_grad = True
            
        # ë§ˆì§€ë§‰ ì •ê·œí™” ë ˆì´ì–´ë„ í•™ìŠµ ê°€ëŠ¥í•˜ê²Œ ì„¤ì •
        for param in model.features.norm5.parameters():
            param.requires_grad = True
        
        # ë¶„ë¥˜ê¸° ë ˆì´ì–´ êµì²´ (ë“œë¡­ì•„ì›ƒ í¬í•¨)
        num_features = model.classifier.in_features
        model.classifier = nn.Sequential(
            nn.Dropout(0.3),  # ê³¼ì í•© ë°©ì§€
            nn.Linear(num_features, 2)  # ì´ì§„ ë¶„ë¥˜
        )
        
        # ìƒˆë¡œ ì¶”ê°€í•œ ë ˆì´ì–´ ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
        model.classifier.apply(self.init_weights)
        
        model = model.to(self.device)
        print("âœ… ëª¨ë¸ ìƒì„± ë° GPU ë¡œë”© ì™„ë£Œ")
        
        return model
    
    def train_model(self, model, dataloaders, criterion, optimizer, scheduler, ratio, 
                   metric='val_f1', patience=10):
        """
        ëª¨ë¸ í•™ìŠµ í•¨ìˆ˜
        
        Args:
            model: í•™ìŠµí•  ëª¨ë¸
            dataloaders: ë°ì´í„° ë¡œë” ë”•ì…”ë„ˆë¦¬
            criterion: ì†ì‹¤ í•¨ìˆ˜
            optimizer: ì˜µí‹°ë§ˆì´ì €
            scheduler: í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬
            ratio: í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ë°ì´í„° ë¹„ìœ¨
            metric: ëª¨ë¸ ì„ íƒ ê¸°ì¤€ ë©”íŠ¸ë¦­
            patience: ì¡°ê¸° ì¢…ë£Œ ì¸ë‚´ì‹¬
            
        Returns:
            tuple: (í•™ìŠµ ê¸°ë¡, ìµœê³  ì„±ëŠ¥, ìµœê³  ì„±ëŠ¥ ì—í­)
        """
        print(f"ğŸš€ ëª¨ë¸ í•™ìŠµ ì‹œì‘ (ë°ì´í„° ë¹„ìœ¨: {ratio*100:.0f}%)")
        
        history = {
            'train_loss': [], 'train_acc': [],
            'val_loss': [], 'val_acc': [],
            'val_precision': [], 'val_recall': [], 'val_f1': []
        }
        
        start_time = time.time()
        best_model_wts = model.state_dict()
        best_score = 0.0
        best_epoch = 0
        early_stop_counter = 0
        
        print(f"ğŸ“Š í•™ìŠµ ì„¤ì •:")
        print(f"  - ì—í­: {self.num_epochs}")
        print(f"  - ì¡°ê¸° ì¢…ë£Œ ê¸°ì¤€: {metric}")
        print(f"  - ì¡°ê¸° ì¢…ë£Œ ì¸ë‚´ì‹¬: {patience}")
        
        for epoch in range(self.num_epochs):
            print(f'\nğŸ“… Epoch {epoch+1}/{self.num_epochs}')
            print('-' * 50)
            
            # ê° ì—í­ì€ í•™ìŠµê³¼ ê²€ì¦ ë‹¨ê³„ë¥¼ ê°€ì§
            for phase in ['train', 'val']:
                if phase == 'train':
                    model.train()  # í•™ìŠµ ëª¨ë“œ
                else:
                    model.eval()   # í‰ê°€ ëª¨ë“œ
                    
                running_loss = 0.0
                all_preds = []
                all_labels = []
                
                # ì§„í–‰ë¥  í‘œì‹œë¥¼ ìœ„í•œ tqdm
                pbar = tqdm(dataloaders[phase], desc=f'{phase.upper()}')
                
                # ë°ì´í„° ë°˜ë³µ
                for inputs, labels in pbar:
                    inputs = inputs.to(self.device)
                    labels = labels.to(self.device)
                    
                    # ë§¤ê°œë³€ìˆ˜ ê·¸ë˜ë””ì–¸íŠ¸ ì´ˆê¸°í™”
                    optimizer.zero_grad()
                    
                    # ìˆœì „íŒŒ
                    with torch.set_grad_enabled(phase == 'train'):
                        outputs = model(inputs)
                        _, preds = torch.max(outputs, 1)
                        loss = criterion(outputs, labels)
                        
                        # í•™ìŠµ ë‹¨ê³„ì—ì„œë§Œ ì—­ì „íŒŒ + ìµœì í™”
                        if phase == 'train':
                            loss.backward()
                            # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ (ê³¼ì í•© ë°©ì§€)
                            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                            optimizer.step()
                            
                    # í†µê³„ ìˆ˜ì§‘
                    running_loss += loss.item() * inputs.size(0)
                    all_preds.extend(preds.cpu().numpy())
                    all_labels.extend(labels.cpu().numpy())
                    
                    # ì§„í–‰ë¥  í‘œì‹œ ì—…ë°ì´íŠ¸
                    pbar.set_postfix({'Loss': f'{loss.item():.4f}'})
                
                # ì—í­ ì„±ëŠ¥ ê³„ì‚°
                epoch_loss = running_loss / len(dataloaders[phase].dataset)
                epoch_acc = accuracy_score(all_labels, all_preds)
                
                print(f'{phase.upper()} - Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}')
                
                # ê²€ì¦ ë‹¨ê³„ì—ì„œ ì¶”ê°€ ë©”íŠ¸ë¦­ ê³„ì‚°
                if phase == 'val':
                    precision = precision_score(all_labels, all_preds, average='binary', zero_division=0)
                    recall = recall_score(all_labels, all_preds, average='binary', zero_division=0)
                    f1 = f1_score(all_labels, all_preds, average='binary', zero_division=0)
                    
                    print(f'VAL - Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}')
                    
                    # ê¸°ë¡ ì €ì¥
                    history['val_loss'].append(epoch_loss)
                    history['val_acc'].append(epoch_acc)
                    history['val_precision'].append(precision)
                    history['val_recall'].append(recall)
                    history['val_f1'].append(f1)
                    
                    # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
                    if scheduler is not None:
                        scheduler.step(epoch_loss)
                        current_lr = optimizer.param_groups[0]['lr']
                        print(f'ğŸ“‰ í˜„ì¬ í•™ìŠµë¥ : {current_lr:.6f}')
                    
                    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì²´í¬
                    if metric == 'val_acc':
                        current_score = epoch_acc
                    elif metric == 'val_precision':
                        current_score = precision
                    elif metric == 'val_recall':
                        current_score = recall
                    elif metric == 'val_f1':
                        current_score = f1
                    else:
                        current_score = f1  # ê¸°ë³¸ê°’
                    
                    if current_score > best_score:
                        best_score = current_score
                        best_epoch = epoch
                        best_model_wts = model.state_dict().copy()
                        print(f"ğŸ‰ ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥! (ì—í­ {epoch+1}, {metric} = {current_score:.4f})")
                        early_stop_counter = 0
                    else:
                        early_stop_counter += 1
                        print(f"â³ ì„±ëŠ¥ í–¥ìƒ ì—†ìŒ: {early_stop_counter}/{patience}")
                        
                        if early_stop_counter >= patience:
                            print(f"ğŸ›‘ ì¡°ê¸° ì¢…ë£Œ: {patience}ë²ˆ ì—°ì† ì„±ëŠ¥ í–¥ìƒ ì—†ìŒ")
                            break
                else:
                    # í›ˆë ¨ ë‹¨ê³„ ê¸°ë¡
                    history['train_loss'].append(epoch_loss)
                    history['train_acc'].append(epoch_acc)
            
            # ì¡°ê¸° ì¢…ë£Œ í™•ì¸
            if early_stop_counter >= patience:
                print(f"ğŸ í•™ìŠµì„ {epoch+1}ë²ˆì§¸ ì—í­ì—ì„œ ì¡°ê¸° ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
        
        # í•™ìŠµ ì™„ë£Œ ì •ë³´
        time_elapsed = time.time() - start_time
        print(f'\nâ° í•™ìŠµ ì™„ë£Œ: {time_elapsed // 60:.0f}ë¶„ {time_elapsed % 60:.0f}ì´ˆ')
        print(f'ğŸ† ìµœê³  ì„±ëŠ¥: ì—í­ {best_epoch+1}, {metric} = {best_score:.4f}')
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
        model_save_path = os.path.join(self.output_dir, f'densenet121_{int(ratio*100)}.pth')
        torch.save(best_model_wts, model_save_path)
        print(f"ğŸ’¾ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥: {model_save_path}")
        
        # í•™ìŠµ ê¸°ë¡ ì €ì¥
        history_save_path = os.path.join(self.output_dir, f'training_history_{int(ratio*100)}.json')
        with open(history_save_path, 'w') as f:
            # numpy ë°°ì—´ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ì—¬ JSON ì§ë ¬í™” ê°€ëŠ¥í•˜ê²Œ ë§Œë“¦
            json_history = {k: [float(x) for x in v] for k, v in history.items()}
            json.dump(json_history, f, indent=2)
        print(f"ğŸ“Š í•™ìŠµ ê¸°ë¡ ì €ì¥: {history_save_path}")
        
        # ëª¨ë¸ì„ ìµœê³  ì„±ëŠ¥ ìƒíƒœë¡œ ë³µì›
        model.load_state_dict(best_model_wts)
        
        return history, best_score, best_epoch+1
    
    def plot_training_history(self, history, ratio):
        """í•™ìŠµ ê¸°ë¡ ì‹œê°í™”"""
        try:
            fig, axes = plt.subplots(2, 2, figsize=(15, 10))
            fig.suptitle(f'Training History - {int(ratio*100)}% Data', fontsize=16)
            
            # Loss ê·¸ë˜í”„
            axes[0, 0].plot(history['train_loss'], label='Train Loss', color='blue')
            axes[0, 0].plot(history['val_loss'], label='Val Loss', color='red')
            axes[0, 0].set_title('Loss')
            axes[0, 0].set_xlabel('Epoch')
            axes[0, 0].set_ylabel('Loss')
            axes[0, 0].legend()
            axes[0, 0].grid(True)
            
            # Accuracy ê·¸ë˜í”„
            axes[0, 1].plot(history['train_acc'], label='Train Acc', color='blue')
            axes[0, 1].plot(history['val_acc'], label='Val Acc', color='red')
            axes[0, 1].set_title('Accuracy')
            axes[0, 1].set_xlabel('Epoch')
            axes[0, 1].set_ylabel('Accuracy')
            axes[0, 1].legend()
            axes[0, 1].grid(True)
            
            # Precision, Recall, F1 ê·¸ë˜í”„
            axes[1, 0].plot(history['val_precision'], label='Precision', color='green')
            axes[1, 0].plot(history['val_recall'], label='Recall', color='orange')
            axes[1, 0].plot(history['val_f1'], label='F1-Score', color='purple')
            axes[1, 0].set_title('Validation Metrics')
            axes[1, 0].set_xlabel('Epoch')
            axes[1, 0].set_ylabel('Score')
            axes[1, 0].legend()
            axes[1, 0].grid(True)
            
            # ìµœì¢… ì„±ëŠ¥ í‘œì‹œ
            final_text = f"""Final Performance:
            Accuracy: {history['val_acc'][-1]:.4f}
            Precision: {history['val_precision'][-1]:.4f}
            Recall: {history['val_recall'][-1]:.4f}
            F1-Score: {history['val_f1'][-1]:.4f}"""
            
            axes[1, 1].text(0.1, 0.5, final_text, fontsize=12, verticalalignment='center')
            axes[1, 1].set_title('Final Performance')
            axes[1, 1].axis('off')
            
            plt.tight_layout()
            plot_path = os.path.join(self.output_dir, f'training_plot_{int(ratio*100)}.png')
            plt.savefig(plot_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            print(f"ğŸ“ˆ í•™ìŠµ ê·¸ë˜í”„ ì €ì¥: {plot_path}")
            
        except Exception as e:
            print(f"âš ï¸ ê·¸ë˜í”„ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
    
    def train_with_data_ratio(self, ratios=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0], 
                             metric='val_f1'):
        """
        ë°ì´í„° ë¹„ìœ¨ì„ ì¡°ì ˆí•˜ë©° ëª¨ë¸ í•™ìŠµ ìˆ˜í–‰
        
        Args:
            ratios: ì‚¬ìš©í•  ë°ì´í„° ë¹„ìœ¨ ë¦¬ìŠ¤íŠ¸
            metric: ëª¨ë¸ ì„ íƒ ê¸°ì¤€ ë©”íŠ¸ë¦­
            
        Returns:
            dict: í•™ìŠµ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        print("="*80)
        print("ğŸ¯ Classification ëª¨ë¸ í•™ìŠµ ì‹œì‘")
        print("="*80)
        
        # ì „ì²´ ë°ì´í„°ì…‹ ë¡œë“œ
        print("ğŸ“‚ ë°ì´í„°ì…‹ ë¡œë”© ì¤‘...")
        try:
            full_dataset = ObjectDataset(self.class0_dir, self.class1_dir, 
                                       transform=self.data_transforms['train'])
        except Exception as e:
            print(f"âŒ ë°ì´í„°ì…‹ ë¡œë”© ì‹¤íŒ¨: {str(e)}")
            return {}
        
        # í´ë˜ìŠ¤ë³„ ì¸ë±ìŠ¤ ì¶”ì¶œ
        class_indices = {0: [], 1: []}
        for i, (_, label) in enumerate(full_dataset.samples):
            class_indices[label].append(i)
        
        print(f"ğŸ“Š í´ë˜ìŠ¤ë³„ ë°ì´í„° ë¶„í¬:")
        print(f"  - í´ë˜ìŠ¤ 0: {len(class_indices[0])}ê°œ ì´ë¯¸ì§€")
        print(f"  - í´ë˜ìŠ¤ 1: {len(class_indices[1])}ê°œ ì´ë¯¸ì§€")
        
        # ë°ì´í„°ê°€ ë„ˆë¬´ ì ì€ ê²½ìš° ê²½ê³ 
        total_samples = len(class_indices[0]) + len(class_indices[1])
        if total_samples < 100:
            print("âš ï¸ ê²½ê³ : ë°ì´í„° ìˆ˜ê°€ ë§¤ìš° ì ìŠµë‹ˆë‹¤(100ê°œ ë¯¸ë§Œ). ì¶”ê°€ ë°ì´í„° ìˆ˜ì§‘ì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
        
        val_ratio = 0.15  # ê²€ì¦ ì„¸íŠ¸ ë¹„ìœ¨
        results = {'ratios': [], 'best_scores': [], 'best_epochs': [], 'histories': []}
        
        print(f"\nğŸ”„ í•™ìŠµí•  ë°ì´í„° ë¹„ìœ¨: {ratios}")
        print(f"ğŸ¯ ëª¨ë¸ ì„ íƒ ê¸°ì¤€: {metric}")
        
        # ê° ë°ì´í„° ë¹„ìœ¨ì— ëŒ€í•´ í•™ìŠµ ì‹¤í–‰
        for i, ratio in enumerate(ratios):
            print(f"\n{'='*60}")
            print(f"ğŸ“Š ë°ì´í„° ë¹„ìœ¨: {ratio*100:.0f}% ({i+1}/{len(ratios)})")
            print(f"{'='*60}")
            
            try:
                # ê° í´ë˜ìŠ¤ì—ì„œ ë¹„ìœ¨ë§Œí¼ ë°ì´í„° ì„ íƒ
                train_indices = []
                for label in [0, 1]:
                    class_size = len(class_indices[label])
                    num_samples = int(class_size * ratio)
                    
                    if num_samples < 3:
                        print(f"âš ï¸ ê²½ê³ : í´ë˜ìŠ¤ {label}ì˜ ë°ì´í„°ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤ ({num_samples}ê°œ)")
                    
                    # ì ì¸µì  ì„ íƒ (í•­ìƒ ë™ì¼í•œ ìˆœì„œ)
                    train_indices.extend(class_indices[label][:num_samples])
                
                # í•™ìŠµ/ê²€ì¦ ë¶„í• 
                val_size = max(int(len(train_indices) * val_ratio), 2)  # ìµœì†Œ 2ê°œëŠ” ê²€ì¦ìš©
                train_size = len(train_indices) - val_size
                
                # ì¸ë±ìŠ¤ ì„ê¸° (ì¬í˜„ ê°€ëŠ¥í•˜ë„ë¡)
                random.shuffle(train_indices)
                train_subset = Subset(full_dataset, train_indices[val_size:])
                
                # ê²€ì¦ ë°ì´í„°ì…‹ (ë³„ë„ ë³€í™˜ ì ìš©)
                val_dataset = ObjectDataset(self.class0_dir, self.class1_dir, 
                                          transform=self.data_transforms['val'])
                val_subset = Subset(val_dataset, train_indices[:val_size])
                
                print(f"ğŸ“Š ë°ì´í„° ë¶„í• :")
                print(f"  - í•™ìŠµ ì„¸íŠ¸: {len(train_subset)}ê°œ")
                print(f"  - ê²€ì¦ ì„¸íŠ¸: {len(val_subset)}ê°œ")
                
                # ë°°ì¹˜ í¬ê¸° ì¡°ì • (ë°ì´í„°ê°€ ì ì€ ê²½ìš°)
                actual_batch_size = min(self.batch_size, len(train_subset) // 2)
                actual_batch_size = max(actual_batch_size, 1)  # ìµœì†Œ 1ê°œ
                
                # ë°ì´í„° ë¡œë” ìƒì„±
                dataloaders = {
                    'train': DataLoader(train_subset, batch_size=actual_batch_size, 
                                      shuffle=True, num_workers=4, pin_memory=True),
                    'val': DataLoader(val_subset, batch_size=actual_batch_size, 
                                    shuffle=False, num_workers=4, pin_memory=True)
                }
                
                print(f"âš™ï¸ ì‹¤ì œ ë°°ì¹˜ í¬ê¸°: {actual_batch_size}")
                
                # ëª¨ë¸ ìƒì„±
                model = self.create_model()
                
                # í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬ë¥¼ ìœ„í•œ ê°€ì¤‘ì¹˜ ê³„ì‚°
                class_samples = [len(class_indices[0]), len(class_indices[1])]
                if min(class_samples) > 0:
                    weights = [len(full_dataset) / (2 * count) if count > 0 else 1.0 
                             for count in class_samples]
                    class_weights = torch.FloatTensor(weights).to(self.device)
                    criterion = nn.CrossEntropyLoss(weight=class_weights)
                    print(f"âš–ï¸ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì ìš©: {[f'{w:.3f}' for w in weights]}")
                else:
                    criterion = nn.CrossEntropyLoss()
                
                # ì˜µí‹°ë§ˆì´ì € ì„¤ì •
                trainable_params = [param for param in model.parameters() if param.requires_grad]
                optimizer = optim.Adam(
                    trainable_params, 
                    lr=0.0005,  # ë‚®ì€ í•™ìŠµë¥ 
                    weight_decay=0.0001  # L2 ì •ê·œí™”
                )
                
                # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬
                scheduler = ReduceLROnPlateau(
                    optimizer,
                    mode='min',
                    factor=0.5,
                    patience=5,
                    verbose=True,
                    min_lr=1e-7
                )
                
                # ëª¨ë¸ í•™ìŠµ
                history, best_score, best_epoch = self.train_model(
                    model, dataloaders, criterion, optimizer, scheduler, 
                    ratio, metric, patience=10
                )
                
                # ê²°ê³¼ ì €ì¥
                results['ratios'].append(ratio)
                results['best_scores'].append(best_score)
                results['best_epochs'].append(best_epoch)
                results['histories'].append(history)
                
                # í•™ìŠµ ê·¸ë˜í”„ ì €ì¥
                self.plot_training_history(history, ratio)
                
                print(f"âœ… {ratio*100:.0f}% ë°ì´í„° í•™ìŠµ ì™„ë£Œ:")
                print(f"   - ìµœê³  ì„±ëŠ¥ ({metric}): {best_score:.4f} (ì—í­ {best_epoch})")
                
            except Exception as e:
                print(f"âŒ {ratio*100:.0f}% ë°ì´í„° í•™ìŠµ ì‹¤íŒ¨: {str(e)}")
                import traceback
                traceback.print_exc()
                
                # ì‹¤íŒ¨í•œ ê²½ìš°ì—ë„ ê²°ê³¼ì— ê¸°ë¡ (0ìœ¼ë¡œ)
                results['ratios'].append(ratio)
                results['best_scores'].append(0.0)
                results['best_epochs'].append(0)
                results['histories'].append({})
        
        # ì „ì²´ ê²°ê³¼ ìš”ì•½
        print("\n" + "="*80)
        print("ğŸ† Classification í•™ìŠµ ê²°ê³¼ ìš”ì•½")
        print("="*80)
        
        for i, ratio in enumerate(results['ratios']):
            if i < len(results['best_scores']):
                score = results['best_scores'][i]
                epoch = results['best_epochs'][i]
                status = "âœ…" if score > 0 else "âŒ"
                print(f"{status} ë°ì´í„° ë¹„ìœ¨ {ratio*100:3.0f}%: {metric} = {score:.4f} (ì—í­ {epoch})")
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì •ë³´
        if results['best_scores']:
            best_idx = np.argmax(results['best_scores'])
            best_ratio = results['ratios'][best_idx]
            best_performance = results['best_scores'][best_idx]
            print(f"\nğŸ¯ ìµœê³  ì„±ëŠ¥: {best_ratio*100:.0f}% ë°ì´í„°, {metric} = {best_performance:.4f}")
        
        # ê²°ê³¼ ì €ì¥
        results_path = os.path.join(self.output_dir, 'classification_results.json')
        with open(results_path, 'w') as f:
            # íˆìŠ¤í† ë¦¬ëŠ” ë„ˆë¬´ í¬ë¯€ë¡œ ì œì™¸í•˜ê³  ì €ì¥
            save_results = {k: v for k, v in results.items() if k != 'histories'}
            json.dump(save_results, f, indent=2)
        print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {results_path}")
        
        return results

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    trainer = ClassificationTrainer(
        class0_dir='./manual_labeling_output/class0',
        class1_dir='./manual_labeling_output/class1',
        output_dir='./models/classification',
        batch_size=16,
        num_epochs=20,  # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ë‚®ì¶˜ ê°’
        gpu_num=0
    )
    
    # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì ì€ ë¹„ìœ¨ë§Œ ì‹¤í–‰
    results = trainer.train_with_data_ratio(ratios=[0.1, 0.5, 1.0])
    print(f"í•™ìŠµ ì™„ë£Œ! ê²°ê³¼: {results}")