# modules/iterative_processor.py
"""
Iterative Active Learning Process ëª¨ë“ˆ
ì—¬ëŸ¬ YOLO ëª¨ë¸ì— ëŒ€í•´ Classification ëª¨ë¸ê³¼ ê²°í•©ëœ ë°˜ë³µì  í•™ìŠµì„ ìˆ˜í–‰í•˜ëŠ” ëª¨ë“ˆ
"""

import os
import glob
import time
import traceback
import pandas as pd
from datetime import datetime
from pathlib import Path
import shutil
import json
from tqdm import tqdm

# ë¡œì»¬ ëª¨ë“ˆ ì„í¬íŠ¸
from modules.yolo_active_learning import YOLOActiveLearning

class IterativeProcessor:
    """
    Iterative Active Learning í”„ë¡œì„¸ìŠ¤ ê´€ë¦¬ì
    ì—¬ëŸ¬ YOLO ëª¨ë¸ì— ëŒ€í•´ Classification ëª¨ë¸ì„ í™œìš©í•œ ë°˜ë³µì  í•™ìŠµì„ ìˆ˜í–‰
    """
    
    def __init__(self, yolo_models_dir, classifier_path, image_dir, label_dir, output_dir,
                 conf_threshold=0.25, iou_threshold=0.5, class_conf_threshold=0.5, 
                 max_cycles=10, gpu_num=0):
        """
        Iterative Processor ì´ˆê¸°í™”
        
        Args:
            yolo_models_dir (str): YOLO ëª¨ë¸ë“¤ì´ ì €ì¥ëœ ë””ë ‰í† ë¦¬
            classifier_path (str): ë¶„ë¥˜ ëª¨ë¸ ê²½ë¡œ
            image_dir (str): ì´ë¯¸ì§€ ë°ì´í„°ì…‹ ê²½ë¡œ
            label_dir (str): ì •ë‹µ ë¼ë²¨ ê²½ë¡œ
            output_dir (str): ê²°ê³¼ ì €ì¥ ê²½ë¡œ
            conf_threshold (float): ê°ì²´ ê²€ì¶œ ì‹ ë¢°ë„ ì„ê³„ê°’
            iou_threshold (float): IoU ì„ê³„ê°’
            class_conf_threshold (float): ë¶„ë¥˜ ëª¨ë¸ ì‹ ë¢°ë„ ì„ê³„ê°’
            max_cycles (int): ìµœëŒ€ í•™ìŠµ ë°˜ë³µ íšŸìˆ˜
            gpu_num (int): ì‚¬ìš©í•  GPU ë²ˆí˜¸
        """
        self.yolo_models_dir = yolo_models_dir
        self.classifier_path = classifier_path
        self.image_dir = image_dir
        self.label_dir = label_dir
        self.output_dir = output_dir
        self.conf_threshold = conf_threshold
        self.iou_threshold = iou_threshold
        self.class_conf_threshold = class_conf_threshold
        self.max_cycles = max_cycles
        self.gpu_num = gpu_num
        
        # ì„¤ì • ì •ë³´ ì¶œë ¥
        print("ğŸ”§ Iterative Processor ì„¤ì •:")
        print(f"  - YOLO ëª¨ë¸ ë””ë ‰í† ë¦¬: {yolo_models_dir}")
        print(f"  - ë¶„ë¥˜ ëª¨ë¸: {classifier_path}")
        print(f"  - ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬: {image_dir}")
        print(f"  - ë¼ë²¨ ë””ë ‰í† ë¦¬: {label_dir}")
        print(f"  - ê²°ê³¼ ì €ì¥: {output_dir}")
        print(f"  - ìµœëŒ€ ì‚¬ì´í´: {max_cycles}")
        print(f"  - GPU: {gpu_num}")
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(output_dir, exist_ok=True)
        
        # ëª¨ë¸ ê²€ì¦
        self._validate_inputs()
        
        print("âœ… Iterative Processor ì´ˆê¸°í™” ì™„ë£Œ")
        
    def _validate_inputs(self):
        """ì…ë ¥ íŒŒë¼ë¯¸í„° ê²€ì¦"""
        # YOLO ëª¨ë¸ ë””ë ‰í† ë¦¬ í™•ì¸
        if not os.path.exists(self.yolo_models_dir):
            raise FileNotFoundError(f"YOLO ëª¨ë¸ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.yolo_models_dir}")
            
        # ë¶„ë¥˜ ëª¨ë¸ íŒŒì¼ í™•ì¸
        if not os.path.exists(self.classifier_path):
            raise FileNotFoundError(f"ë¶„ë¥˜ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.classifier_path}")
            
        # ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ í™•ì¸
        if not os.path.exists(self.image_dir):
            raise FileNotFoundError(f"ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.image_dir}")
            
        # ë¼ë²¨ ë””ë ‰í† ë¦¬ í™•ì¸
        if not os.path.exists(self.label_dir):
            raise FileNotFoundError(f"ë¼ë²¨ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.label_dir}")
        
        print("âœ… ì…ë ¥ íŒŒë¼ë¯¸í„° ê²€ì¦ ì™„ë£Œ")
    
    def get_yolo_models(self):
        """YOLO ëª¨ë¸ íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
        print("ğŸ” YOLO ëª¨ë¸ íŒŒì¼ ê²€ìƒ‰ ì¤‘...")
        
        # .pt íŒŒì¼ ê²€ìƒ‰
        model_paths = glob.glob(os.path.join(self.yolo_models_dir, "*.pt"))
        
        if not model_paths:
            raise Exception(f"YOLO ëª¨ë¸(.pt íŒŒì¼)ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.yolo_models_dir}")
        
        # íŒŒì¼ëª… ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        model_paths.sort()
        
        print(f"ğŸ“Š ì´ {len(model_paths)}ê°œì˜ YOLO ëª¨ë¸ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤:")
        for i, path in enumerate(model_paths):
            model_name = os.path.basename(path)
            file_size = os.path.getsize(path) / (1024 * 1024)  # MB
            print(f"  {i+1:2d}. {model_name} ({file_size:.1f} MB)")
        
        return model_paths
    
    def run_single_experiment(self, model_path):
        """
        ë‹¨ì¼ YOLO ëª¨ë¸ì— ëŒ€í•œ Active Learning ì‹¤í—˜ ì‹¤í–‰
        
        Args:
            model_path (str): YOLO ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
            
        Returns:
            dict: ì‹¤í—˜ ê²°ê³¼ ì •ë³´
        """
        model_filename = os.path.basename(model_path)
        model_name = os.path.splitext(model_filename)[0]
        
        # ëª¨ë¸ë³„ ì¶œë ¥ ë””ë ‰í† ë¦¬
        model_output_dir = os.path.join(self.output_dir, model_name)
        
        print(f"\n{'='*80}")
        print(f"ğŸš€ YOLO ëª¨ë¸ ì‹¤í—˜ ì‹œì‘: {model_filename}")
        print(f"ğŸ“ ê²°ê³¼ ì €ì¥ ê²½ë¡œ: {model_output_dir}")
        print(f"{'='*80}")
        
        # ì‹¤í—˜ ì‹œì‘ ì‹œê°„ ê¸°ë¡
        start_time = time.time()
        
        try:
            # YOLOActiveLearning ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
            print("ğŸ—ï¸ Active Learning ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
            active_learning = YOLOActiveLearning(
                model_path=model_path,
                classifier_path=self.classifier_path,
                image_dir=self.image_dir,
                label_dir=self.label_dir,
                output_dir=model_output_dir,
                conf_threshold=self.conf_threshold,
                iou_threshold=self.iou_threshold,
                class_conf_threshold=self.class_conf_threshold,
                max_cycles=self.max_cycles,
                gpu_num=self.gpu_num,
                use_classifier=True  # ë¶„ë¥˜ ëª¨ë¸ ì‚¬ìš©
            )
            
            print("âœ… Active Learning ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
            
            # Active Learning í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰
            print("ğŸ”„ Active Learning í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰ ì¤‘...")
            active_learning.run()
            
            # ì‹¤í–‰ ì‹œê°„ ê³„ì‚°
            elapsed_time = time.time() - start_time
            
            print(f"âœ… YOLO ëª¨ë¸ {model_filename} ì‹¤í—˜ ì™„ë£Œ")
            print(f"â° ì‹¤í–‰ ì‹œê°„: {elapsed_time/60:.1f}ë¶„")
            
            return {
                "status": "ì™„ë£Œ",
                "message": "ì„±ê³µì ìœ¼ë¡œ ì‹¤í–‰ë¨",
                "model_name": model_name,
                "model_path": model_path,
                "output_dir": model_output_dir,
                "elapsed_time": elapsed_time,
                "cycles_completed": self.max_cycles
            }
            
        except Exception as e:
            elapsed_time = time.time() - start_time
            error_message = f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            error_detail = traceback.format_exc()
            
            print(f"\n{'!'*80}")
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {model_filename} ì‹¤í—˜ ì¤‘ ì˜¤ë¥˜")
            print(f"ğŸ’¥ ì˜¤ë¥˜ ë©”ì‹œì§€: {str(e)}")
            print(f"â° ì‹¤í–‰ ì‹œê°„: {elapsed_time/60:.1f}ë¶„")
            print(f"{'!'*80}")
            
            # ì˜¤ë¥˜ ë¡œê·¸ ì €ì¥
            self._save_error_log(model_output_dir, error_message, error_detail, elapsed_time)
            
            return {
                "status": "ì‹¤íŒ¨",
                "message": str(e),
                "model_name": model_name,
                "model_path": model_path,
                "output_dir": model_output_dir,
                "elapsed_time": elapsed_time,
                "error_detail": error_detail
            }
    
    def _save_error_log(self, output_dir, error_message, error_detail, elapsed_time):
        """ì˜¤ë¥˜ ë¡œê·¸ ì €ì¥"""
        try:
            os.makedirs(output_dir, exist_ok=True)
            error_log_dir = os.path.join(output_dir, "error_logs")
            os.makedirs(error_log_dir, exist_ok=True)
            
            error_log_path = os.path.join(error_log_dir, "error.log")
            with open(error_log_path, "w", encoding='utf-8') as f:
                f.write(f"ì‹¤í—˜ ì‹¤í–‰ ì˜¤ë¥˜ ë¡œê·¸\n")
                f.write(f"="*50 + "\n")
                f.write(f"ì˜¤ë¥˜ ë°œìƒ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"ì‹¤í–‰ ì‹œê°„: {elapsed_time/60:.1f}ë¶„\n")
                f.write(f"ì˜¤ë¥˜ ë©”ì‹œì§€: {error_message}\n\n")
                f.write(f"ìƒì„¸ ì˜¤ë¥˜ ë‚´ìš©:\n")
                f.write(f"-"*50 + "\n")
                f.write(f"{error_detail}\n")
            
            print(f"ğŸ“ ì˜¤ë¥˜ ë¡œê·¸ ì €ì¥: {error_log_path}")
            
        except Exception as log_error:
            print(f"âš ï¸ ì˜¤ë¥˜ ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨: {str(log_error)}")
    
    def collect_metrics(self, experiment_results):
        """
        ëª¨ë“  ì‹¤í—˜ ê²°ê³¼ì—ì„œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë° í†µí•©
        
        Args:
            experiment_results (dict): ì‹¤í—˜ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
            
        Returns:
            pandas.DataFrame: í†µí•©ëœ ë©”íŠ¸ë¦­ ë°ì´í„°í”„ë ˆì„
        """
        print("ğŸ“Š ì‹¤í—˜ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì¤‘...")
        
        combined_metrics_df = pd.DataFrame()
        collected_count = 0
        
        for model_name, result in experiment_results.items():
            if result["status"] == "ì™„ë£Œ":
                metrics_file = os.path.join(result["output_dir"], "performance_metrics.csv")
                
                if os.path.exists(metrics_file):
                    try:
                        model_metrics = pd.read_csv(metrics_file)
                        
                        # ëª¨ë¸ ì´ë¦„ì´ ì—†ìœ¼ë©´ ì¶”ê°€
                        if 'Model' not in model_metrics.columns:
                            model_metrics['Model'] = model_name
                        
                        # ì‹¤í—˜ ì •ë³´ ì¶”ê°€
                        model_metrics['Experiment_Status'] = 'Success'
                        model_metrics['Elapsed_Time'] = result.get("elapsed_time", 0)
                        
                        combined_metrics_df = pd.concat([combined_metrics_df, model_metrics], 
                                                       ignore_index=True)
                        collected_count += 1
                        
                        print(f"  âœ… {model_name}: {len(model_metrics)}ê°œ ì‚¬ì´í´ ë©”íŠ¸ë¦­ ìˆ˜ì§‘")
                        
                    except Exception as e:
                        print(f"  âŒ {model_name}: ë©”íŠ¸ë¦­ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜ - {str(e)}")
                else:
                    print(f"  âš ï¸ {model_name}: ë©”íŠ¸ë¦­ íŒŒì¼ ì—†ìŒ")
            else:
                print(f"  ğŸ’¥ {model_name}: ì‹¤í—˜ ì‹¤íŒ¨ë¡œ ë©”íŠ¸ë¦­ ì—†ìŒ")
        
        print(f"ğŸ“ˆ ì´ {collected_count}ê°œ ëª¨ë¸ì˜ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì™„ë£Œ")
        
        return combined_metrics_df
    
    def create_comparison_tables(self, combined_metrics_df, output_dir):
        """ì„±ëŠ¥ ë¹„êµ í…Œì´ë¸” ìƒì„±"""
        print("ğŸ“Š ì„±ëŠ¥ ë¹„êµ í…Œì´ë¸” ìƒì„± ì¤‘...")
        
        if combined_metrics_df.empty:
            print("âš ï¸ ë©”íŠ¸ë¦­ ë°ì´í„°ê°€ ì—†ì–´ ë¹„êµ í…Œì´ë¸”ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return
        
        try:
            # ë©”íŠ¸ë¦­ë³„ ë¹„êµ í…Œì´ë¸” ìƒì„±
            metrics_to_compare = ['mAP50', 'Precision', 'Recall', 'F1-Score', 
                                'Detected_Objects', 'Filtered_Objects']
            
            for metric in metrics_to_compare:
                if metric in combined_metrics_df.columns:
                    try:
                        # ì‚¬ì´í´ê³¼ ëª¨ë¸ì„ ê¸°ì¤€ìœ¼ë¡œ í”¼ë²— í…Œì´ë¸” ìƒì„±
                        pivot_df = combined_metrics_df.pivot_table(
                            index='Cycle', 
                            columns='Model', 
                            values=metric,
                            aggfunc='mean'
                        )
                        
                        # í…Œì´ë¸” ì €ì¥
                        table_file = os.path.join(output_dir, f"{metric}_comparison_table.csv")
                        pivot_df.to_csv(table_file)
                        
                        print(f"  ğŸ“„ {metric} ë¹„êµ í…Œì´ë¸” ì €ì¥: {table_file}")
                        
                        # ìµœì¢… ì‚¬ì´í´ ì„±ëŠ¥ ìš”ì•½
                        if not pivot_df.empty:
                            final_cycle = pivot_df.index.max()
                            final_performance = pivot_df.loc[final_cycle].sort_values(ascending=False)
                            
                            print(f"  ğŸ† {metric} ìµœì¢… ì„±ëŠ¥ (ì‚¬ì´í´ {final_cycle}):")
                            for model, score in final_performance.head(3).items():
                                if not pd.isna(score):
                                    print(f"    {model}: {score:.4f}")
                        
                    except Exception as e:
                        print(f"  âŒ {metric} í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨: {str(e)}")
                else:
                    print(f"  âš ï¸ {metric} ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤")
            
            # ì „ì²´ ë©”íŠ¸ë¦­ ìš”ì•½ í…Œì´ë¸”
            summary_file = os.path.join(output_dir, "performance_summary.csv")
            combined_metrics_df.to_csv(summary_file, index=False)
            print(f"ğŸ“‹ ì „ì²´ ì„±ëŠ¥ ìš”ì•½ ì €ì¥: {summary_file}")
            
        except Exception as e:
            print(f"âŒ ë¹„êµ í…Œì´ë¸” ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
    
    def generate_experiment_report(self, experiment_results, combined_metrics_df, output_dir):
        """ì‹¤í—˜ ê²°ê³¼ ì¢…í•© ë³´ê³ ì„œ ìƒì„±"""
        print("ğŸ“ ì‹¤í—˜ ë³´ê³ ì„œ ìƒì„± ì¤‘...")
        
        report_file = os.path.join(output_dir, "experiment_report.txt")
        
        try:
            with open(report_file, "w", encoding='utf-8') as f:
                # ë³´ê³ ì„œ í—¤ë”
                f.write("="*80 + "\n")
                f.write("ITERATIVE ACTIVE LEARNING ì‹¤í—˜ ë³´ê³ ì„œ\n")
                f.write("="*80 + "\n")
                f.write(f"ìƒì„± ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"ì‹¤í—˜ ì„¤ì •:\n")
                f.write(f"  - ìµœëŒ€ ì‚¬ì´í´: {self.max_cycles}\n")
                f.write(f"  - ì‹ ë¢°ë„ ì„ê³„ê°’: {self.conf_threshold}\n")
                f.write(f"  - IoU ì„ê³„ê°’: {self.iou_threshold}\n")
                f.write(f"  - ë¶„ë¥˜ ì‹ ë¢°ë„ ì„ê³„ê°’: {self.class_conf_threshold}\n")
                f.write(f"  - GPU: {self.gpu_num}\n")
                f.write("\n")
                
                # ì‹¤í—˜ ê²°ê³¼ ìš”ì•½
                total_models = len(experiment_results)
                successful_count = sum(1 for r in experiment_results.values() if r["status"] == "ì™„ë£Œ")
                failed_count = total_models - successful_count
                
                f.write("ğŸ“Š ì‹¤í—˜ ê²°ê³¼ ìš”ì•½\n")
                f.write("-"*50 + "\n")
                f.write(f"ì´ ëª¨ë¸ ìˆ˜: {total_models}ê°œ\n")
                f.write(f"ì„±ê³µ: {successful_count}ê°œ ({successful_count/total_models*100:.1f}%)\n")
                f.write(f"ì‹¤íŒ¨: {failed_count}ê°œ ({failed_count/total_models*100:.1f}%)\n")
                f.write("\n")
                
                # ê° ëª¨ë¸ë³„ ìƒì„¸ ê²°ê³¼
                f.write("ğŸ“‹ ëª¨ë¸ë³„ ìƒì„¸ ê²°ê³¼\n")
                f.write("-"*50 + "\n")
                
                for model_name, result in experiment_results.items():
                    status = result["status"]
                    elapsed_time = result.get("elapsed_time", 0)
                    
                    f.write(f"\nğŸ”¹ {model_name}\n")
                    f.write(f"   ìƒíƒœ: {status}\n")
                    f.write(f"   ì‹¤í–‰ì‹œê°„: {elapsed_time/60:.1f}ë¶„\n")
                    
                    if status == "ì™„ë£Œ":
                        f.write(f"   ê²°ê³¼ ë””ë ‰í† ë¦¬: {result['output_dir']}\n")
                        f.write(f"   ì™„ë£Œ ì‚¬ì´í´: {result.get('cycles_completed', 'N/A')}\n")
                    else:
                        f.write(f"   ì˜¤ë¥˜ ë©”ì‹œì§€: {result['message']}\n")
                
                # ì„±ëŠ¥ í†µê³„ (ë©”íŠ¸ë¦­ì´ ìˆëŠ” ê²½ìš°)
                if not combined_metrics_df.empty:
                    f.write("\n\nğŸ“ˆ ì„±ëŠ¥ í†µê³„\n")
                    f.write("-"*50 + "\n")
                    
                    # ìµœì¢… ì‚¬ì´í´ì˜ í‰ê·  ì„±ëŠ¥
                    final_cycle = combined_metrics_df['Cycle'].max()
                    final_metrics = combined_metrics_df[combined_metrics_df['Cycle'] == final_cycle]
                    
                    if not final_metrics.empty:
                        f.write(f"ìµœì¢… ì‚¬ì´í´ ({final_cycle}) í‰ê·  ì„±ëŠ¥:\n")
                        
                        for metric in ['mAP50', 'Precision', 'Recall', 'F1-Score']:
                            if metric in final_metrics.columns:
                                avg_score = final_metrics[metric].mean()
                                f.write(f"  {metric}: {avg_score:.4f}\n")
                        
                        f.write(f"\nê°ì²´ íƒì§€ í†µê³„:\n")
                        if 'Detected_Objects' in final_metrics.columns:
                            total_detected = final_metrics['Detected_Objects'].sum()
                            f.write(f"  ì´ íƒì§€ ê°ì²´: {total_detected:,}ê°œ\n")
                        
                        if 'Filtered_Objects' in final_metrics.columns:
                            total_filtered = final_metrics['Filtered_Objects'].sum()
                            f.write(f"  í•„í„°ë§ëœ ê°ì²´: {total_filtered:,}ê°œ\n")
                            
                            if total_detected > 0:
                                filter_rate = total_filtered / (total_detected + total_filtered) * 100
                                f.write(f"  í•„í„°ë§ ë¹„ìœ¨: {filter_rate:.1f}%\n")
                    
                    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸
                    if 'F1-Score' in combined_metrics_df.columns:
                        best_performance = combined_metrics_df.loc[combined_metrics_df['F1-Score'].idxmax()]
                        f.write(f"\nğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸:\n")
                        f.write(f"  ëª¨ë¸: {best_performance['Model']}\n")
                        f.write(f"  ì‚¬ì´í´: {best_performance['Cycle']}\n")
                        f.write(f"  F1-Score: {best_performance['F1-Score']:.4f}\n")
                
                # ì‹¤í—˜ íŒŒì¼ ëª©ë¡
                f.write(f"\n\nğŸ“ ìƒì„±ëœ íŒŒì¼\n")
                f.write("-"*50 + "\n")
                f.write(f"- ì‹¤í—˜ ë³´ê³ ì„œ: {report_file}\n")
                f.write(f"- ì„±ëŠ¥ ìš”ì•½: {os.path.join(output_dir, 'performance_summary.csv')}\n")
                f.write(f"- ë¹„êµ í…Œì´ë¸”: {output_dir}/*_comparison_table.csv\n")
                f.write(f"- ëª¨ë¸ë³„ ê²°ê³¼: {output_dir}/[model_name]/\n")
                
            print(f"ğŸ“„ ì‹¤í—˜ ë³´ê³ ì„œ ì €ì¥: {report_file}")
            
        except Exception as e:
            print(f"âŒ ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {str(e)}")
    
    def save_experiment_config(self, output_dir):
        """ì‹¤í—˜ ì„¤ì •ì„ JSON íŒŒì¼ë¡œ ì €ì¥"""
        config = {
            "experiment_datetime": datetime.now().isoformat(),
            "yolo_models_dir": self.yolo_models_dir,
            "classifier_path": self.classifier_path,
            "image_dir": self.image_dir,
            "label_dir": self.label_dir,
            "output_dir": self.output_dir,
            "parameters": {
                "conf_threshold": self.conf_threshold,
                "iou_threshold": self.iou_threshold,
                "class_conf_threshold": self.class_conf_threshold,
                "max_cycles": self.max_cycles,
                "gpu_num": self.gpu_num
            }
        }
        
        config_file = os.path.join(output_dir, "experiment_config.json")
        try:
            with open(config_file, "w") as f:
                json.dump(config, f, indent=2)
            print(f"âš™ï¸ ì‹¤í—˜ ì„¤ì • ì €ì¥: {config_file}")
        except Exception as e:
            print(f"âš ï¸ ì„¤ì • ì €ì¥ ì‹¤íŒ¨: {str(e)}")
    
    def run_iterative_experiments(self):
        """
        ëª¨ë“  YOLO ëª¨ë¸ì— ëŒ€í•´ Iterative ì‹¤í—˜ ì‹¤í–‰
        
        Returns:
            dict: ì¢…í•© ì‹¤í—˜ ê²°ê³¼
        """
        print("="*80)
        print("ğŸš€ ITERATIVE ACTIVE LEARNING ì‹¤í—˜ ì‹œì‘")
        print("="*80)
        
        # ì‹¤í—˜ ì‹œì‘ ì‹œê°„ ê¸°ë¡
        total_start_time = time.time()
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ì´ìš©í•œ ì‹¤í—˜ ë””ë ‰í† ë¦¬ ìƒì„±
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        experiment_dir = os.path.join(self.output_dir, f"iterative_experiment_{timestamp}")
        os.makedirs(experiment_dir, exist_ok=True)
        
        print(f"ğŸ“ ì‹¤í—˜ ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬: {experiment_dir}")
        
        # ì‹¤í—˜ ì„¤ì • ì €ì¥
        self.save_experiment_config(experiment_dir)
        
        # YOLO ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        try:
            model_paths = self.get_yolo_models()
        except Exception as e:
            print(f"âŒ YOLO ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {str(e)}")
            return {"error": str(e)}
        
        # ì‹¤í—˜ ê²°ê³¼ ì €ì¥ìš© ë”•ì…”ë„ˆë¦¬
        experiment_results = {}
        
        # ì§„í–‰ë¥  í‘œì‹œë¥¼ ìœ„í•œ tqdm
        print(f"\nğŸ”„ {len(model_paths)}ê°œ ëª¨ë¸ì— ëŒ€í•œ ì‹¤í—˜ ì‹œì‘")
        
        model_pbar = tqdm(model_paths, desc="Models", unit="model")
        
        # ê° YOLO ëª¨ë¸ì— ëŒ€í•´ ì‹¤í—˜ ìˆ˜í–‰
        for i, model_path in enumerate(model_pbar):
            model_filename = os.path.basename(model_path)
            model_name = os.path.splitext(model_filename)[0]
            
            # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
            model_pbar.set_description(f"Processing {model_name}")
            
            print(f"\nğŸ¯ ì‹¤í—˜ ì§„í–‰: {i+1}/{len(model_paths)} - {model_name}")
            
            # ê°œë³„ ëª¨ë¸ ì‹¤í—˜ ì‹¤í–‰
            result = self.run_single_experiment(model_path)
            experiment_results[model_name] = result
            
            # ì¤‘ê°„ ê²°ê³¼ ì¶œë ¥
            if result["status"] == "ì™„ë£Œ":
                print(f"âœ… {model_name} ì™„ë£Œ ({result['elapsed_time']/60:.1f}ë¶„)")
            else:
                print(f"âŒ {model_name} ì‹¤íŒ¨: {result['message']}")
        
        model_pbar.close()
        
        # ì „ì²´ ì‹¤í—˜ ì‹œê°„ ê³„ì‚°
        total_elapsed_time = time.time() - total_start_time
        
        print(f"\nâ° ì „ì²´ ì‹¤í—˜ ì‹œê°„: {total_elapsed_time/60:.1f}ë¶„")
        
        # ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë° í†µí•©
        print("\nğŸ“Š ì‹¤í—˜ ê²°ê³¼ ë¶„ì„ ì¤‘...")
        combined_metrics_df = self.collect_metrics(experiment_results)
        
        # í†µí•© ë©”íŠ¸ë¦­ ì €ì¥
        if not combined_metrics_df.empty:
            combined_metrics_file = os.path.join(experiment_dir, "combined_performance_metrics.csv")
            combined_metrics_df.to_csv(combined_metrics_file, index=False)
            print(f"ğŸ’¾ í†µí•© ë©”íŠ¸ë¦­ ì €ì¥: {combined_metrics_file}")
            
            # ë¹„êµ í…Œì´ë¸” ìƒì„±
            self.create_comparison_tables(combined_metrics_df, experiment_dir)
        
        # ì¢…í•© ë³´ê³ ì„œ ìƒì„±
        self.generate_experiment_report(experiment_results, combined_metrics_df, experiment_dir)
        
        # ìµœì¢… ê²°ê³¼ ìš”ì•½
        successful_count = sum(1 for r in experiment_results.values() if r["status"] == "ì™„ë£Œ")
        failed_count = len(experiment_results) - successful_count
        
        print("\n" + "="*80)
        print("ğŸ‰ ITERATIVE ACTIVE LEARNING ì‹¤í—˜ ì™„ë£Œ")
        print("="*80)
        print(f"ğŸ“Š ì‹¤í—˜ ê²°ê³¼:")
        print(f"  - ì´ ëª¨ë¸: {len(model_paths)}ê°œ")
        print(f"  - ì„±ê³µ: {successful_count}ê°œ")
        print(f"  - ì‹¤íŒ¨: {failed_count}ê°œ")
        print(f"  - ì„±ê³µë¥ : {successful_count/len(model_paths)*100:.1f}%")
        print(f"â° ì´ ì‹¤í–‰ ì‹œê°„: {total_elapsed_time/60:.1f}ë¶„")
        print(f"ğŸ“ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {experiment_dir}")
        
        if not combined_metrics_df.empty:
            print(f"ğŸ“ˆ ë©”íŠ¸ë¦­ ìš”ì•½:")
            print(f"  - í†µí•© ë©”íŠ¸ë¦­: {len(combined_metrics_df)}ê°œ ê¸°ë¡")
            print(f"  - ë¹„êµ í…Œì´ë¸”: {len([f for f in os.listdir(experiment_dir) if f.endswith('_comparison_table.csv')])}ê°œ")
        
        # ë°˜í™˜í•  ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ êµ¬ì„±
        final_results = {
            "experiment_dir": experiment_dir,
            "results": experiment_results,
            "combined_metrics": combined_metrics_df,
            "summary": {
                "total_models": len(model_paths),
                "successful": successful_count,
                "failed": failed_count,
                "success_rate": successful_count/len(model_paths)*100,
                "total_time_minutes": total_elapsed_time/60,
                "experiments_per_hour": len(model_paths) / (total_elapsed_time / 3600)
            }
        }
        
        return final_results
    
    def cleanup_temp_files(self):
        """ì„ì‹œ íŒŒì¼ ì •ë¦¬"""
        print("ğŸ§¹ ì„ì‹œ íŒŒì¼ ì •ë¦¬ ì¤‘...")
        
        temp_patterns = [
            "runs/detect/*/",  # YOLO í•™ìŠµ ì„ì‹œ ê²°ê³¼
            "*.yaml",          # ì„ì‹œ YAML íŒŒì¼
            "__pycache__/",    # Python ìºì‹œ
        ]
        
        for pattern in temp_patterns:
            try:
                temp_files = glob.glob(pattern)
                for temp_file in temp_files:
                    if os.path.isdir(temp_file):
                        shutil.rmtree(temp_file, ignore_errors=True)
                    else:
                        os.remove(temp_file)
                
                if temp_files:
                    print(f"  ğŸ—‘ï¸ {len(temp_files)}ê°œ {pattern} íŒŒì¼/í´ë” ì •ë¦¬ë¨")
            except Exception as e:
                print(f"  âš ï¸ {pattern} ì •ë¦¬ ì‹¤íŒ¨: {str(e)}")
        
        print("âœ… ì„ì‹œ íŒŒì¼ ì •ë¦¬ ì™„ë£Œ")

def main():
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰ì„ ìœ„í•œ ë©”ì¸ í•¨ìˆ˜"""
    # í…ŒìŠ¤íŠ¸ ì„¤ì •
    processor = IterativeProcessor(
        yolo_models_dir="./models/initial_yolo",
        classifier_path="./models/classification/densenet121_100.pth",
        image_dir="./dataset/images",
        label_dir="./dataset/labels",
        output_dir="./results/iterative_process",
        conf_threshold=0.25,
        iou_threshold=0.5,
        class_conf_threshold=0.5,
        max_cycles=5,  # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ë‚®ì¶¤
        gpu_num=0
    )
    
    # ì‹¤í—˜ ì‹¤í–‰
    results = processor.run_iterative_experiments()
    
    # ê²°ê³¼ ì¶œë ¥
    if "error" not in results:
        print("\nğŸ¯ ì‹¤í—˜ ì™„ë£Œ! ì£¼ìš” ê²°ê³¼:")
        summary = results["summary"]
        print(f"  - ì„±ê³µë¥ : {summary['success_rate']:.1f}%")
        print(f"  - ì´ ì‹œê°„: {summary['total_time_minutes']:.1f}ë¶„")
        print(f"  - ì‹œê°„ë‹¹ ì‹¤í—˜: {summary['experiments_per_hour']:.1f}ê°œ")
    else:
        print(f"âŒ ì‹¤í—˜ ì‹¤íŒ¨: {results['error']}")

if __name__ == "__main__":
    main()